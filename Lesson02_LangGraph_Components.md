### Graphs

![graphs](./screenshots/003.png)

### Cyclic Graphs (å¾ªç¯å›¾)

![Cyclic graphs](./screenshots/005.png)
è¿™ä¸ªå›¾è¢«ç§°ä¸º Cyclic Graphsï¼ˆå¾ªç¯å›¾ï¼‰ï¼Œæ˜¯å› ä¸ºæ•´ä¸ªæµç¨‹å›¾ä¸­å­˜åœ¨ä¸€ä¸ªé—­ç¯ï¼ˆloopï¼‰ï¼Œä¹Ÿå°±æ˜¯ï¼š

ğŸ” å›¾ä¸­å¾ªç¯çš„è·¯å¾„ï¼š

1. ç”¨æˆ·æå‡ºé—®é¢˜ï¼ˆUserï¼‰
2. ç³»ç»Ÿæ‹¼æ¥ Promptï¼ˆå« system prompt å’Œ user inputï¼‰
3. LLM æ ¹æ®å½“å‰ä¿¡æ¯ç”Ÿæˆ Tool Call
4. åˆ†æ”¯åˆ¤æ–­æ˜¯å¦éœ€è¦è°ƒç”¨å·¥å…·ï¼š
5. è‹¥éœ€è¦ï¼Œè¿›å…¥ action èŠ‚ç‚¹
6. å·¥å…·è°ƒç”¨åç”Ÿæˆ Observationï¼ˆæ¯”å¦‚ Obs: 37ï¼‰

Observation ä½œä¸ºè¾“å…¥å†æ¬¡é€å›ç»™ LLM

è¿›å…¥ä¸‹ä¸€è½®æ¨ç†ï¼Œç›´åˆ°æœ€ç»ˆè¿”å› Final Answer

### å¦‚ä½•ç”¨ LangGraph å®ç° ReAct

```py
from typing import TypedDict, Annotated
import operator
from langgraph.graph import StateGraph, END
from langchain_core.messages import SystemMessage, ToolMessage


class Agent:

    def __init__(self, model, tools, system=""):
        # Store optional system prompt
        self.system = system

        # Create a LangGraph state graph with AgentState (which contains messages)
        graph = StateGraph(AgentState)

        # Add LLM node, which handles generating Thought/Action/Answer
        graph.add_node("llm", self.call_openai)

        # Add Action node, which executes tool calls
        graph.add_node("action", self.take_action)

        # Add conditional logic:
        # If the LLM response contains tool calls, go to action node.
        # Otherwise, finish the graph.
        graph.add_conditional_edges(
            "llm", self.exists_action, {True: "action", False: END}
        )

        # Create a loop: after tool execution, go back to the LLM
        graph.add_edge("action", "llm")

        # Set the starting node to be the LLM
        graph.set_entry_point("llm")

        # Compile the graph so it's ready to run
        self.graph = graph.compile()

        # print ASCII diagram
        # self.graph.get_graph().print_ascii()

        # or get mermaid diagram
        # print(self.graph.get_graph().draw_mermaid())

        # Store tools as a name â†’ tool map for dispatch
        self.tools = {t.name: t for t in tools}

        # Bind tools to the LLM so it knows the available actions
        self.model = model.bind_tools(tools)

    def exists_action(self, state: AgentState):
        # Check if the latest message from AIMessage includes tool calls
        result = state["messages"][-1]
        return len(result.tool_calls) > 0

    def call_openai(self, state: AgentState):
        # Prepare message history
        messages = state["messages"]

        # Prepend system prompt if available
        # since it's not stored in state["messages"] by default.
        if self.system:
            messages = [SystemMessage(content=self.system)] + messages

        # Send messages to the LLM and get a new response (which may include tool calls)
        message = self.model.invoke(messages)

        # Return the new assistant message in the required update format
        return {"messages": [message]}

    def take_action(self, state: AgentState):
        # Extract tool calls from the latest AI message.
        #
        # For example, the latest message (AIMessage) may look like this:
        #
        #   Tool Name: None
        #   Tool Calls:
        #     [
        #         {
        #             'name': 'average_dog_weight',
        #             'args': {'name': 'Border Collie'},
        #             'id': 'call_403boG61cKmxfUA59KtLYApF',
        #             'type': 'tool_call'
        #         },
        #         {
        #             'name': 'average_dog_weight',
        #             'args': {'name': 'Scottish Terrier'},
        #             'id': 'call_0zJOoGb4xZI3dNNuIo15Vuou',
        #             'type': 'tool_call'
        #         }
        #     ]
        #   Content: (empty at this stage)
        #
        # This indicates the LLM has decided to invoke two tool calls,
        # which we now need to execute.
        tool_calls = state["messages"][-1].tool_calls
        results = []

        for t in tool_calls:
            print(f"Calling: {t}")

            # Check if the tool name is valid
            if not t["name"] in self.tools:
                print("\n ....bad tool name....")
                result = f"bad tool name ${t["name"]}, retry"  # Let the LLM handle the error
            else:
                # Execute the corresponding tool with the provided arguments
                result = self.tools[t["name"]].invoke(t["args"])

            # Wrap the result in a ToolMessage so the LLM can observe it
            results.append(
                ToolMessage(
                    tool_call_id=t["id"],  # Echo the tool call ID for tracking
                    name=t["name"],
                    content=str(result),
                )
            )

        print("Back to the model!")

        # Return all tool observations in the expected format
        return {"messages": results}
```

è¿™æ®µä»£ç ç¡®å®å®ç°äº† ReActï¼ˆReasoning + Actingï¼‰æ¨¡å¼ï¼Œä½¿ç”¨äº† LangGraph æ¥ç®¡ç†æ¨ç†ï¼ˆThoughtï¼‰ä¸è¡ŒåŠ¨ï¼ˆActionï¼‰ä¹‹é—´çš„å¾ªç¯ã€‚

âœ… ä»€ä¹ˆæ˜¯ ReAct æ¨¡å¼ï¼Ÿ
ReActï¼ˆReasoning + Actingï¼‰æ˜¯ä¸€ç§ agent ç­–ç•¥ï¼ŒLLM åå¤è¿›è¡Œï¼š

- Reasoningï¼ˆæ¨ç†ï¼‰ï¼šåˆ†æå½“å‰ä»»åŠ¡ï¼Œå†³å®šä¸‹ä¸€æ­¥è¡ŒåŠ¨ã€‚
- Actingï¼ˆæ‰§è¡Œï¼‰ï¼šè°ƒç”¨å¤–éƒ¨å·¥å…·ï¼ˆå¦‚æœç´¢ã€APIã€è®¡ç®—ï¼‰æ¥è·å–ä¿¡æ¯ã€‚
- Observationï¼ˆè§‚å¯Ÿï¼‰ï¼šæ¥æ”¶å·¥å…·æ‰§è¡Œç»“æœï¼Œå†æ¬¡æ¨ç†ï¼Œç›´åˆ°æœ€ç»ˆå›ç­”ã€‚

âœ… ä½ çš„ä»£ç æ˜¯å¦‚ä½•ä½“ç° ReAct çš„ï¼Ÿ
| ReAct é˜¶æ®µ | å¯¹åº”ä»£ç  | è¯´æ˜ |
| ----------------------- | ----------------------------------------------------- | ------------------------------------- |
| **System Prompt è®¾ç½®è§’è‰²** | `SystemMessage(content=self.system)` | è®¾ç½® LLM çš„è¡Œä¸ºé£æ ¼ã€ç›®æ ‡ç­‰ã€‚ |
| **Reasoningï¼ˆæ¨ç†ï¼‰** | `call_openai()` â†’ `self.model.invoke(messages)` | LLM ç”Ÿæˆå·¥å…·è°ƒç”¨è¯·æ±‚ (`tool_calls`) æˆ–ç›´æ¥ç”Ÿæˆæœ€ç»ˆå›ç­” |
| **Actingï¼ˆè¡ŒåŠ¨ï¼‰** | `take_action()` ä¸­ `self.tools[t["name"]].invoke(...)` | å®é™…è°ƒç”¨å·¥å…·ï¼Œå¦‚è®¡ç®—ä½“é‡ã€æŸ¥è¯¢ç­‰ |
| **Observationï¼ˆè§‚å¯Ÿå·¥å…·è¿”å›ï¼‰** | `ToolMessage(content=str(result))` | æŠŠå·¥å…·æ‰§è¡Œç»“æœåŒ…è£…åè¿”å›ç»™ LLMï¼Œç”¨äºç»§ç»­ Reasoning |
| **å¾ªç¯æ‰§è¡Œ** | `graph.add_edge("action", "llm")` | å·¥å…·æ‰§è¡Œåå†è°ƒç”¨ LLMï¼Œå®ç°å¾ªç¯ç›´åˆ°æ²¡æœ‰ tool_calls |

âœ… æ˜¯ä¸æ˜¯æ‰€æœ‰ LangChain çš„ messages éƒ½è¦æŒ‰ [System, Human, AI, Tool] é¡ºåºï¼Ÿ
ä¸å¿…é¡»æ˜¯ä¸¥æ ¼çš„é¡ºåºï¼Œä½† ReAct ç±»å‹çš„å¯¹è¯é€šå¸¸æŒ‰ç»„ç»“æ„å‡ºç°å¤šè½®ï¼š

```
[SystemMessage]
â†’ HumanMessage
â†’ AIMessage (with tool_calls)
â†’ ToolMessage (tool result)
â†’ AIMessage (reasoning with observation)
â†’ (å¯é€‰æ›´å¤šè½®)
â†’ Final AIMessage (ç›´æ¥å›ç­”)
```

ä½ ä¹Ÿå¯ä»¥æœ‰å¤šä¸ª Human â†’ AI â†’ Tool â†’ AI çš„ç»„åˆï¼Œå½¢æˆæ›´å¤æ‚çš„ reasoning + acting loopã€‚è¿™æ˜¯ LangGraph å¼ºå¤§çš„åœ°æ–¹â€”â€”å¯ä»¥æ¸…æ™°è¡¨è¾¾å¤šè½® agent æµç¨‹ã€‚

âœ… ç»“è®º
æ˜¯çš„ï¼Œè¿™æ®µä»£ç å·²ç»æ˜¯ä¸€ä¸ªå…¸å‹çš„ ReAct agent å®ç°ï¼š

- å®ƒä½¿ç”¨ LLM æ¨ç†æ˜¯å¦éœ€è¦è°ƒç”¨å·¥å…·ã€‚
- ä½¿ç”¨å·¥å…·è·å–å¤–éƒ¨ä¿¡æ¯ã€‚
- å°†ç»“æœè¿”å›ç»™ LLM ç»§ç»­æ€è€ƒã€‚

ä½¿ç”¨ LangGraph å®ç°æ˜¾å¼æ§åˆ¶æµç¨‹å’Œå¾ªç¯é€»è¾‘ï¼Œæ¯”ä¼ ç»Ÿ ReAct å®ç°æ›´å¯æ§ã€‚

æˆ‘ä»¬æ¥æ–°å»ºä¸€ä¸ª agent

```py
prompt = """You are a smart research assistant. Use the search engine to look up information. \
You are allowed to make multiple calls (either together or in sequence). \
Only look up information when you are sure of what you want. \
If you need to look up some information before asking a follow up question, you are allowed to do that!
"""

model = ChatOpenAI(model="gpt-4o-mini")

tools = [calculate, average_dog_weight]
my_bot = Agent(model, tools, system=prompt)
```

ç„¶åè°ƒç”¨å®ƒ

```py
from langchain_core.messages import HumanMessage

question = """I have 2 dogs, a Border Collie and a Scottish Terrier. \
What is their combined weight"""

initial_state = {"messages": [HumanMessage(content=question)]}

# Run the graph â€” LangGraph will automatically manage tool calls and LLM turns
final_state = my_bot.graph.invoke(initial_state)
```

å¯ä»¥çœ‹åˆ°å®Œæ•´çš„ messages (LangChain é»˜è®¤ä¸åŒ…å« SystemMessage)

```py
[0] HUMAN
Tool Name: None
Tool Calls: None
Content:
I have 2 dogs, a Border Collie and a Scottish Terrier. What is their combined weight

[1] AI
Tool Name: None
Tool Calls: [{'name': 'average_dog_weight', 'args': {'name': 'Border Collie'}, 'id': 'call_403boG61cKmxfUA59KtLYApF', 'type': 'tool_call'}, {'name': 'average_dog_weight', 'args': {'name': 'Scottish Terrier'}, 'id': 'call_0zJOoGb4xZI3dNNuIo15Vuou', 'type': 'tool_call'}]
Content:


[2] TOOL
Tool Name: average_dog_weight
Tool Calls: None
Content:
a Border Collies average weight is 37 lbs

[3] TOOL
Tool Name: average_dog_weight
...
Tool Calls: []
Content:
The combined average weight of your Border Collie and Scottish Terrier is 57 pounds.
```

å¦‚æœå°† prompt è°ƒæ•´åˆ°æ›´åŠ ç¬¦åˆ ReAct çš„æ ¼å¼

```py
prompt = """You are a helpful and intelligent assistant that uses a step-by-step process to solve problems.
You have access to tools and should use the following format in your natural language response:

Thought: Describe your reasoning
Action: tool name
Action Input: JSON-formatted arguments
Observation: (after tool output)
Final Answer: Your final conclusion

You must always include these sections in your response content when using a tool.

Available tools:
- average_dog_weight
- calculate
"""
```

å¾—åˆ°çš„ output ä¼šæ˜¯

```py
Calling: {'name': 'average_dog_weight', 'args': {'name': 'Border Collie'}, 'id': 'call_5F4NK3FrdNFmeeSyxDCtf0QR', 'type': 'tool_call'}
Calling: {'name': 'average_dog_weight', 'args': {'name': 'Scottish Terrier'}, 'id': 'call_7mbJ5v5ZXmsDUaHkfFNPvYx8', 'type': 'tool_call'}
Back to the model!
Calling: {'name': 'calculate', 'args': {'what': '37 + 20'}, 'id': 'call_i5nvFAxnbdq5VFS06LqMLBXd', 'type': 'tool_call'}
Back to the model!
[0] HUMAN
Tool Name: None
Tool Calls: None
Content:
     I have 2 dogs, a Border Collie and a Scottish Terrier. What is their combined weight
--------------------------------------------------
[1] AI
Tool Name: None
Tool Calls: [{'name': 'average_dog_weight', 'args': {'name': 'Border Collie'}, 'id': 'call_5F4NK3FrdNFmeeSyxDCtf0QR', 'type': 'tool_call'}, {'name': 'average_dog_weight', 'args': {'name': 'Scottish Terrier'}, 'id': 'call_7mbJ5v5ZXmsDUaHkfFNPvYx8', 'type': 'tool_call'}]
Content: None
--------------------------------------------------
[2] TOOL
Tool Name: average_dog_weight
Tool Calls: None
Content:
     a Border Collies average weight is 37 lbs
--------------------------------------------------
[3] TOOL
Tool Name: average_dog_weight
Tool Calls: None
Content:
     Scottish Terriers average 20 lbs
--------------------------------------------------
[4] AI
Tool Name: None
Tool Calls: [{'name': 'calculate', 'args': {'what': '37 + 20'}, 'id': 'call_i5nvFAxnbdq5VFS06LqMLBXd', 'type': 'tool_call'}]
Content:
  ğŸ§  Thought: I need to sum the average weights of a Border Collie and a Scottish Terrier to find their combined weight. The average weight of a Border Collie is 37 lbs, and the average weight of a Scottish Terrier is 20 lbs.

  ğŸ› ï¸  Action: Calculate the combined weight.
  ğŸ“¦ Action Input: JSON-formatted arguments: {"what": "37 + 20"}

  ğŸ‘€ Observation: The computation of their combined weight is necessary to arrive at the final answer.

  ğŸ› ï¸  Action: functions.calculate
  ğŸ“¦ Action Input: {"what":"37 + 20"}
--------------------------------------------------
[5] TOOL
Tool Name: calculate
Tool Calls: None
Content:
     57
--------------------------------------------------
[6] AI
Tool Name: None
Tool Calls: []
Content:
  âœ… Final Answer: The combined weight of your Border Collie and Scottish Terrier is 57 lbs.
--------------------------------------------------
```

#### ä¸ºä»€ä¹ˆ LangChain é»˜è®¤ä¸åŒ…å« SystemMessage

å¯ä»¥çœ‹åˆ°ï¼Œä¸‹é¢çš„ call_openai tool,æ¯æ¬¡éƒ½è¦ Prepend system prompt

```py
def call_openai(self, state: AgentState):
        # Prepare message history
        messages = state["messages"]

        # Prepend system prompt if available
        if self.system:
            messages = [SystemMessage(content=self.system)] + messages

        # Send messages to the LLM and get a new response (which may include tool calls)
        message = self.model.invoke(messages)

        # Return the new assistant message in the required update format
        return {"messages": [message]}
```

âœ… state["messages"] ä¸­ä¸åŒ…å«ç³»ç»Ÿæç¤ºï¼ˆSystemMessageï¼‰

- åœ¨ LangGraph / LangChain çš„äº¤äº’ä¸­ï¼ŒSystemMessage é€šå¸¸ ä¸ä¼šè‡ªåŠ¨ä¿å­˜åœ¨çŠ¶æ€çš„æ¶ˆæ¯å†å²ä¸­ã€‚
- state["messages"] åªä¼šå­˜å‚¨ç”¨æˆ·ï¼ˆHumanMessageï¼‰ã€AIï¼ˆAIMessageï¼‰ã€å·¥å…·è°ƒç”¨ï¼ˆToolMessageï¼‰ç­‰äº¤äº’å†…å®¹ã€‚
- å¦‚æœä½ å¸Œæœ›æ¨¡å‹æŒç»­æŒ‰ç…§æŸä¸ªè§’è‰²æˆ–ä¸Šä¸‹æ–‡æ€ç»´æ–¹å¼å›ç­”ï¼ˆæ¯”å¦‚â€œä½ æ˜¯ä¸€ä¸ªèªæ˜çš„ç ”ç©¶åŠ©æ‰‹â€ï¼‰ï¼Œæ¯æ¬¡è°ƒç”¨æ¨¡å‹å‰å¿…é¡»æ‰‹åŠ¨åŠ ä¸Š SystemMessageã€‚

âœ… ä¸ºä»€ä¹ˆä¸æŠŠ SystemMessage æ°¸ä¹…ä¿å­˜åˆ°æ¶ˆæ¯å†å²ä¸­ï¼Ÿ

1. è¿™æ˜¯å‡ºäºè®¾è®¡è€ƒè™‘ï¼š

- SystemMessage é€šå¸¸æ˜¯â€œé™æ€é…ç½®â€è€Œéå¯¹è¯çš„ä¸€éƒ¨åˆ†
- å®ƒä¸æ˜¯å¯¹è¯æµç¨‹ä¸­çš„ä¸€è½®è¾“å…¥è¾“å‡ºï¼Œè€Œæ˜¯å¯¹æ¨¡å‹è¡Œä¸ºçš„â€œè¯´æ˜æ€§è®¾å®šâ€ã€‚
- ä¿ç•™åœ¨ self.system ä¸­ï¼Œä¸´æ—¶æ‹¼æ¥è¿›ä¸Šä¸‹æ–‡ï¼Œæ˜¯ä¸ºäº†æ›´æ¸…æ™°åœ°åŒºåˆ†ç³»ç»Ÿè¡Œä¸ºä¸å®é™…å¯¹è¯å†å²ã€‚

2. é¿å…å†—ä½™

- å¦‚æœä½ æ¯è½®éƒ½æŠŠ SystemMessage æ”¾å…¥ state["messages"]ï¼Œé‚£ä¸‹ä¸€è½®åˆä¼šå†æ¬¡ prependï¼Œå°±ä¼šé‡å¤ã€‚

âœ… æ€»ç»“

```py
system:
messages = [SystemMessage(content=self.system)] + messages
```

å°±æ˜¯ä¸ºäº†ç¡®ä¿æ¨¡å‹æ¯æ¬¡è°ƒç”¨æ—¶éƒ½â€œè®°ä½â€ä½ çš„è§’è‰²è®¾å®šæˆ–æŒ‡ä»¤ï¼Œä½†åˆä¸ä¼šæ±¡æŸ“ state["messages"] çš„ç»“æ„ã€‚

å¦‚æœä½ ç¡®å®å¸Œæœ›æŠŠç³»ç»Ÿæç¤ºä¹Ÿä¿å­˜åœ¨æ¶ˆæ¯å†å²ä¸­ï¼Œä½ å¯ä»¥æ‰‹åŠ¨è¿™ä¹ˆåšä¸€æ¬¡ï¼Œä¾‹å¦‚åœ¨é¦–æ¬¡åˆå§‹åŒ– state æ—¶åŠ å…¥å®ƒã€‚ä½†è¿™ä¸æ˜¯é»˜è®¤è¡Œä¸ºã€‚éœ€è¦ä½ è‡ªå·±ç»´æŠ¤å®ƒçš„ä½ç½®å’Œæ˜¯å¦é‡å¤æ·»åŠ ã€‚

![ReAct via LangGraph](./screenshots/004.png)

### ä¸æ˜¯æ‰€æœ‰ LangChain çš„ messages éƒ½å¿…é¡»ä¸¥æ ¼æ˜¯è¿™å‡ ä¸ªç±»å‹æˆ–ä¸¥æ ¼[SystemMessage(...), user_msg, assistant_msg, tool_msg]è¿™ä¸ªé¡ºåºï¼Œä¸è¿‡åœ¨å…¸å‹çš„ tool-calling å¯¹è¯åœºæ™¯ä¸­ï¼Œç¡®å®å¤§è‡´éµå¾ªè¿™ä¸ªé¡ºåºç»“æ„ï¼Œ ä½†æœ‰çš„å¯ä»¥æ˜¯å¤šè½®å¤šç»„ï¼Œä¾‹å¦‚ï¼š[sys, user, ai, tool, ai, user, ai, tool, ai, ...]

âœ… å¸¸è§çš„ messages ç±»å‹æœ‰ï¼š
| ç±»å‹ | ç±»å | è¯´æ˜ |
| --------------- | --------------- | ---------------------------- |
| `SystemMessage` | `SystemMessage` | æŒ‡å®š LLM è§’è‰²ï¼ˆä¾‹å¦‚è®¾å®šç³»ç»Ÿè¡Œä¸ºã€é£æ ¼ç­‰ï¼‰ |
| `HumanMessage` | `HumanMessage` | ç”¨æˆ·å‘æ¥çš„é—®é¢˜æˆ–å‘½ä»¤ |
| `AIMessage` | `AIMessage` | LLM çš„å›ç­”ï¼ˆå¯åŒ…å« `tool_calls`ï¼‰ |
| `ToolMessage` | `ToolMessage` | å·¥å…·æ‰§è¡Œç»“æœä½œä¸º observationï¼Œå–‚å›ç»™ LLM |

âœ… åœ¨ tool-calling agent åœºæ™¯ä¸‹ï¼Œæ ‡å‡†é¡ºåºæ˜¯ï¼š

```
[SystemMessage]
â†’ HumanMessage (ç”¨æˆ·æé—®)
â†’ AIMessage (æ¨¡å‹ç”Ÿæˆå·¥å…·è°ƒç”¨æŒ‡ä»¤)
â†’ ToolMessage (ä½ ç”¨ Python æ‰§è¡Œåè¿”å›çš„ç»“æœ)
â†’ AIMessage (æ¨¡å‹ç»§ç»­åŸºäº observation æ¨ç†)
...
```

ä¸¾ä¸ªå®Œæ•´ä¾‹å­ï¼š

```py
[
  SystemMessage(content="You are a helpful assistant."),
  HumanMessage(content="How much does a bulldog weigh?"),
  AIMessage(tool_calls=[...]),  # Action: average_dog_weight: Bulldog
  ToolMessage(tool_call_id="xyz", name="average_dog_weight", content="51 lbs"),
  AIMessage(content="A bulldog weighs 51 lbs.")  # final answer
]
```

âœ… ä¸ºä»€ä¹ˆ AIMessage(tool_calls=[...]) ä¼šåŒ…å« tool_callsï¼Ÿ
å› ä¸ºè¿™æ˜¯ OpenAIï¼ˆæˆ–å…¶ä»–æ”¯æŒ tool calling çš„ LLMï¼‰æ¨¡å‹å“åº”ä¸­çš„ç»“æ„åŒ–è¾“å‡ºæ ¼å¼ã€‚

ğŸ” èƒŒæ™¯ï¼šTool Calling æ˜¯æ€ä¹ˆè®¾è®¡çš„ï¼Ÿ
å½“ä½ ä½¿ç”¨ OpenAI çš„ gpt-4, gpt-4o, æˆ– gpt-3.5-turbo-0613 ç­‰æ”¯æŒ tool calling / function calling çš„æ¨¡å‹æ—¶ï¼š

ä½ åœ¨æ¨¡å‹åˆå§‹åŒ–æ—¶ç”¨ .bind_tools(tools) æˆ–æŒ‡å®š tools=[...]

LLM ä¸ç›´æ¥è¿”å›è‡ªç„¶è¯­è¨€å›ç­”ï¼Œè€Œæ˜¯åˆ¤æ–­æ˜¯å¦è¯¥è°ƒç”¨å·¥å…·

å¦‚æœæ¨¡å‹è®¤ä¸ºå®ƒéœ€è¦ä½¿ç”¨æŸä¸ªå·¥å…·ï¼Œå®ƒå°±ä¼šè¿”å›ä¸€æ¡åŒ…å« tool_calls çš„ assistant æ¶ˆæ¯

âœ… tool_calls æ˜¯æ¨¡å‹ä¸»åŠ¨ç”Ÿæˆçš„ç»“æ„åŒ–å­—æ®µ
è¿”å›çš„å†…å®¹å¤§æ¦‚æ˜¯è¿™æ ·çš„ï¼ˆJSON è¡¨ç¤ºï¼‰ï¼š

```py
{
  "role": "assistant",
  "tool_calls": [
    {
      "id": "call_abc123",
      "name": "search",
      "args": {
        "query": "NVIDIA stock price"
      }
    }
  ]
}
```

è¿™ä¸æ˜¯ä½ æ‰‹åŠ¨åŠ è¿›å»çš„ï¼Œè€Œæ˜¯ æ¨¡å‹ç”Ÿæˆçš„ç»“æœï¼ŒLangChain è‡ªåŠ¨å°†å…¶å°è£…æˆ AIMessage(tool_calls=...)ã€‚

âœ… åœ¨ LangChain ä¸­çš„è¡¨ç°ï¼š

```py
AIMessage(tool_calls=[
    {
        "id": "xyz123",
        "name": "calculate",
        "args": {"expression": "3 + 4"}
    }
])
```

ä½ æ”¶åˆ°è¿™æ ·çš„ AIMessage åï¼Œå°±å¯ä»¥ä»ä¸­æå–å·¥å…·åå’Œå‚æ•°ï¼Œè°ƒç”¨æœ¬åœ°å‡½æ•°ï¼Œç„¶åå†æŠŠç»“æœä½œä¸º ToolMessage å–‚å›æ¨¡å‹ã€‚

ğŸ” tool_calls æ˜¯ LLM å‘å‡ºçš„ã€ŒæŒ‡ä»¤ã€
å¯ä»¥ç†è§£ä¸ºï¼š

â€œæˆ‘æ˜¯ assistantï¼Œæˆ‘ä¸èƒ½ç›´æ¥å›ç­”ï¼Œæˆ‘éœ€è¦è°ƒç”¨ calculate() å·¥å…·ï¼Œå‚æ•°æ˜¯ '3 + 4'ã€‚â€

â—ï¸ æ³¨æ„äº‹é¡¹

- ä¸æ˜¯å¿…é¡»æ¯è½®éƒ½åŒ…å« SystemMessageï¼Œä½ å¯ä»¥åªåŠ ä¸€æ¬¡ä½œä¸ºå¼€å¤´è®¾å®šã€‚
- å¤šè½®å¯¹è¯ä¸­ï¼Œmessages ä¼šè¶Šæ¥è¶Šé•¿ï¼Œè®°å½•äº†æ‰€æœ‰å†å²ã€‚
- AIMessageï¼ˆå³ assistant çš„å›å¤ï¼‰æœ‰æ—¶åŒ…å« tool_callsï¼Œæœ‰æ—¶åªæ˜¯è‡ªç„¶è¯­è¨€å›ç­”ã€‚
- ToolMessage åªèƒ½åœ¨è°ƒç”¨å·¥å…·åç”¨ï¼Œä¸èƒ½è‡ªå·±æ‰‹å†™ç»™ LLMï¼Œå®ƒæ˜¯ observationã€‚

### Tavily Search

Tavily Search æ˜¯ä¸€ä¸ªä¸º AI åº”ç”¨å’Œä»£ç†ç³»ç»Ÿè®¾è®¡çš„ Web æœç´¢ API æœåŠ¡ï¼Œå®ƒå…è®¸ä½ é€šè¿‡ç¼–ç¨‹æ–¹å¼è·å–æœ€æ–°ã€ç»“æ„åŒ–çš„ç½‘é¡µæœç´¢ç»“æœï¼Œç‰¹åˆ«é€‚åˆç”¨äºï¼š

- Retrieval-Augmented Generation (RAG)
- ReAct agents / tool-using agents
- AI assistants éœ€è¦è”ç½‘æœç´¢æ—¶
- æ›¿ä»£ä¼ ç»Ÿçš„ Bing / Google æœç´¢ API

âœ… Tavily Search çš„ç‰¹ç‚¹ï¼š
| ç‰¹æ€§ | è¯´æ˜ |
| -------------------- | --------------------------------------- |
| ğŸ” Web search API | ç±»ä¼¼ Google/Bing Search çš„ REST æ¥å£ |
| ğŸ“„ è¿”å›æ‘˜è¦å’Œç½‘é¡µé“¾æ¥ | è¿”å›ç®€æ´æ‘˜è¦ + åŸå§‹ URLï¼Œä¾¿äº AI ä½¿ç”¨ |
| ğŸ§  é¢å‘ LLM/agent ä¼˜åŒ– | ç»“æœç»“æ„åŒ–ï¼Œé€‚åˆåµŒå…¥è¿› AI æç¤ºä¸­ |
| âš¡ å¿«é€Ÿ & å…è´¹å±‚ | æœ‰å…è´¹é…é¢ï¼Œé€‚åˆåŸå‹å¼€å‘å’Œå°é¡¹ç›® |
| ğŸ› ï¸ å¯ä¸ LangChain ç­‰é›†æˆ | LangChain æœ‰å†…ç½® `TavilySearchResults` å·¥å…·ç±» |

```py
import requests

API_KEY = "your_tavily_api_key"
query = "latest news about Nvidia AI chips"

response = requests.post(
    "https://api.tavily.com/search",
    headers={"Authorization": f"Bearer {API_KEY}"},
    json={"query": query, "num_results": 3}
)

for r in response.json()["results"]:
    print(r["title"], r["url"])

```

æˆ–è€…ç»“åˆ LangChain

```py
from langchain.tools.tavily_search import TavilySearchResults

search = TavilySearchResults(k=3)
search.run("What's the latest on OpenAI?")
```

âœ… ä¸ºä»€ä¹ˆå®ƒå—åˆ°å…³æ³¨ï¼Ÿ
å› ä¸ºå®ƒæä¾›äº†ä¸€ä¸ªç®€å•ä½†å¼ºå¤§çš„æ–¹å¼è®© LLM agent èƒ½â€œä¸Šç½‘æŸ¥ä¸œè¥¿â€ï¼Œè€Œä¸åƒ Bing Search API é‚£æ ·å¤æ‚æˆ–å—é™ã€‚
å®ƒåœ¨ RAGã€tool-augmented agentã€LangGraph ç­‰ç°ä»£ AI åº”ç”¨ä¸­å¾ˆå¸¸è§ã€‚

âœ… æ€»ç»“
| é¡¹ç›® | Tavily Search |
| ---- | ------------------------------ |
| ç”¨é€” | AI æœç´¢ä»£ç†å·¥å…· |
| é€‚åˆè° | æ„å»º LLM agentã€RAG ç³»ç»Ÿã€è”ç½‘ AI åŠ©æ‰‹çš„äºº |
| æ¥å£ç±»å‹ | REST APIï¼Œæ”¯æŒ JSON ç»“æ„åŒ–è¿”å› |
| ä¼˜ç‚¹ | å¿«é€Ÿã€æ˜“ç”¨ã€é€‚é… LLMã€å¤šç§é›†æˆï¼ˆLangChain ç­‰ï¼‰ |

```py
from langchain.agents import Tool, initialize_agent
from langchain.agents.agent_types import AgentType
from langchain.tools.tavily_search import TavilySearchResults
from langchain.chat_models import ChatOpenAI

# åˆå§‹åŒ– Tavily æœç´¢å·¥å…·
tavily_tool = TavilySearchResults(k=3)

# åŒ…è£…æˆ LangChain Tool
search_tool = Tool(
    name="web-search",
    func=tavily_tool.run,
    description="Useful for answering questions about current events or recent topics on the internet."
)

# åˆå§‹åŒ– OpenAI æ¨¡å‹
llm = ChatOpenAI(
    model="gpt-4o",  # å¯æ”¹æˆ gpt-3.5-turbo
    temperature=0
)

# åˆå§‹åŒ– agentï¼Œä½¿ç”¨ ReAct é£æ ¼ï¼ˆæ”¯æŒ tool-callingï¼‰
agent = initialize_agent(
    tools=[search_tool],
    llm=llm,
    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,
    verbose=True
)

# è¿è¡Œ agent
question = "What is the latest news about GPT-5?"
response = agent.run(question)

print("\nâœ… Final Answer:\n", response)
```
